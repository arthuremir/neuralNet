{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'filename', 'target', 'target_names']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_iris()\n",
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import permutation\n",
    "\n",
    "shuffled_idx = permutation(range(dataset.data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data = dataset.data[shuffled_idx]\n",
    "dataset.target = dataset.target[shuffled_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y = ohe.fit_transform(dataset.target.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/(X.max(axis=0))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(3,),activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(3,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(x_val[25].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class littleNN(BaseEstimator):\n",
    "    def __init__(self,num_hidden):\n",
    "        self.num_hidden = num_hidden\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def sigmoidPrime(self,x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    def correct_weights(self,hidden_layer,output_layer,y_true,silent=0):\n",
    "              \n",
    "        if not silent:\n",
    "            E = np.sum((output_layer-y_true)**2)\n",
    "            #print('Error: {0}'.format(E))\n",
    "        \n",
    "        delta2 = (output_layer-y_true)*self.sigmoidPrime(output_layer)\n",
    "        self.w2 -= hidden_layer.T.dot(delta2)\n",
    "        \n",
    "        delta1 = delta2.dot(self.w2[:-1].T)*self.sigmoidPrime(hidden_layer[:,:-1])\n",
    "        self.w1 -= self.input_layer.T.dot(delta1) #\n",
    "        \n",
    "        \n",
    "    def fit(self,X,y,num_batches=1,epochs=100):\n",
    "\n",
    "        m = X.shape[0]\n",
    "        num_input = X.shape[1]\n",
    "        num_output = y.shape[1]\n",
    "       \n",
    "        samples_per_batch = m//num_batches\n",
    "        \n",
    "        self.w1 = 2*np.random.rand(num_input+1,self.num_hidden)-1\n",
    "        self.w2 = 2*np.random.rand(self.num_hidden+1,num_output)-1\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            #print('Epoch {0}'.format(i))\n",
    "            k = 0\n",
    "            for sample, y_true in zip(X,y): #zip(X[k:k+samples_per_batch,:], y[k:k+samples_per_batch,:]):\n",
    "                \n",
    "                self.input_layer = np.r_[sample,1].reshape(1,-1)\n",
    "                hidden_layer = self.sigmoid((np.c_[self.input_layer.dot(self.w1),1].reshape(1,-1)))\n",
    "                output_layer = self.sigmoid(hidden_layer.dot(self.w2))\n",
    "                \n",
    "                '''print(self.input_layer.shape)\n",
    "                print(self.w1.shape)\n",
    "                print(hidden_layer.shape)\n",
    "                print(self.w2.shape)\n",
    "                print(output_layer.shape)\n",
    "                print(y_true.shape)'''\n",
    "                \n",
    "                self.correct_weights(hidden_layer,output_layer,y_true.reshape(1,-1))\n",
    "                \n",
    "                #k += samples_per_batch\n",
    "    def predict(self,X):\n",
    "        res = np.zeros((X.shape[0],10))\n",
    "        \n",
    "        for i,sample in enumerate(X):\n",
    "            self.input_layer = np.r_[sample,1].reshape(1,-1)\n",
    "            hidden_layer = self.sigmoid((np.c_[self.input_layer.dot(self.w1),1].reshape(1,-1)))\n",
    "            output_layer = self.sigmoid(hidden_layer.dot(self.w2))\n",
    "            res[i] = np.round(output_layer)\n",
    "            #print(res[i])\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.9       , 1.        , 1.        , 1.        , 0.96666667])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = littleNN(10)\n",
    "cross_val_score(nn,X,y,cv=5,scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.76666667, 0.83333333, 0.86666667, 0.9       , 0.9       ])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "cross_val_score(lr,X,y,cv=5,scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 0.]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = littleNN(3)\n",
    "nn.fit(x_train,y_train)\n",
    "res = nn.predict(x_val)\n",
    "\n",
    "c = 0\n",
    "for i in range(res.shape[0]):\n",
    "    if(all(res[i]==y_val[i])):\n",
    "        c+=1\n",
    "\n",
    "c/res.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False,  True])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3,4])==np.array([1,2,33,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(0.7429,decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68490815, 0.34709096, 0.74357104, 0.00788877],\n",
       "       [0.32005102, 0.35725852, 0.89562805, 0.43805302],\n",
       "       [0.69337965, 0.12506107, 0.4397513 , 0.38414352]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(3,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 2.        , 3.        , 4.        ],\n",
       "       [0.32005102, 0.35725852, 0.89562805, 0.43805302],\n",
       "       [0.69337965, 0.12506107, 0.4397513 , 0.38414352]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.81012658 0.63636364 0.8115942  0.88      ]\n",
      "1 [0.60759494 0.77272727 0.27536232 0.08      ]\n",
      "2 [0.93670886 0.63636364 0.88405797 0.76      ]\n",
      "3 [1.         0.86363636 0.92753623 0.8       ]\n",
      "4 [0.63291139 0.79545455 0.1884058  0.12      ]\n",
      "5 [0.69620253 0.95454545 0.20289855 0.08      ]\n",
      "6 [0.81012658 0.72727273 0.76811594 0.92      ]\n",
      "7 [0.87341772 0.70454545 0.71014493 0.6       ]\n",
      "8 [0.81012658 0.72727273 0.65217391 0.6       ]\n",
      "9 [0.60759494 0.68181818 0.20289855 0.04      ]\n",
      "10 [0.63291139 0.77272727 0.23188406 0.16      ]\n",
      "11 [0.82278481 0.63636364 0.66666667 0.6       ]\n",
      "12 [0.75949367 0.5        0.57971014 0.4       ]\n",
      "13 [0.74683544 0.72727273 0.69565217 0.72      ]\n",
      "14 [0.69620253 0.52272727 0.57971014 0.52      ]\n",
      "15 [0.65822785 0.79545455 0.2173913  0.08      ]\n",
      "16 [0.70886076 0.68181818 0.65217391 0.6       ]\n",
      "17 [0.64556962 0.86363636 0.2173913  0.12      ]\n",
      "18 [0.63291139 0.68181818 0.23188406 0.08      ]\n",
      "19 [0.74683544 0.68181818 0.73913043 0.72      ]\n",
      "20 [0.79746835 0.56818182 0.71014493 0.6       ]\n",
      "21 [0.97468354 0.68181818 0.88405797 0.92      ]\n",
      "22 [0.70886076 0.63636364 0.71014493 0.8       ]\n",
      "23 [0.7721519  0.59090909 0.8115942  0.56      ]\n",
      "24 [0.62025316 0.56818182 0.65217391 0.68      ]\n",
      "25 [0.55696203 0.72727273 0.1884058  0.08      ]\n",
      "26 [0.73417722 0.61363636 0.73913043 0.76      ]\n",
      "27 [0.78481013 0.63636364 0.69565217 0.72      ]\n",
      "28 [0.82278481 0.72727273 0.73913043 0.8       ]\n",
      "29 [0.75949367 0.61363636 0.73913043 0.64      ]\n",
      "30 [0.70886076 0.65909091 0.52173913 0.52      ]\n",
      "31 [0.56962025 0.52272727 0.1884058  0.12      ]\n",
      "32 [0.78481013 0.65909091 0.62318841 0.52      ]\n",
      "33 [0.60759494 0.77272727 0.23188406 0.08      ]\n",
      "34 [0.97468354 0.63636364 0.97101449 0.8       ]\n",
      "35 [0.72151899 0.68181818 0.60869565 0.48      ]\n",
      "36 [0.58227848 0.70454545 0.2173913  0.08      ]\n",
      "37 [0.89873418 0.68181818 0.85507246 0.84      ]\n",
      "38 [0.65822785 0.77272727 0.20289855 0.08      ]\n",
      "39 [0.79746835 0.77272727 0.8115942  0.96      ]\n",
      "40 [0.6835443  0.77272727 0.2173913  0.16      ]\n",
      "41 [0.6835443  0.88636364 0.24637681 0.16      ]\n",
      "42 [0.70886076 0.68181818 0.5942029  0.52      ]\n",
      "43 [0.64556962 0.86363636 0.23188406 0.08      ]\n",
      "44 [0.86075949 0.72727273 0.85507246 0.92      ]\n",
      "45 [0.97468354 0.86363636 0.97101449 0.88      ]\n",
      "46 [0.82278481 0.68181818 0.79710145 0.72      ]\n",
      "47 [0.58227848 0.72727273 0.20289855 0.08      ]\n",
      "48 [0.63291139 0.77272727 0.2173913  0.08      ]\n",
      "49 [0.84810127 0.70454545 0.63768116 0.56      ]\n",
      "50 [0.64556962 0.77272727 0.2173913  0.08      ]\n",
      "51 [0.84810127 0.68181818 0.72463768 0.68      ]\n",
      "52 [0.73417722 0.59090909 0.57971014 0.48      ]\n",
      "53 [0.6835443  0.84090909 0.2173913  0.08      ]\n",
      "54 [0.73417722 0.63636364 0.73913043 0.96      ]\n",
      "55 [0.79746835 0.75       0.86956522 1.        ]\n",
      "56 [0.72151899 0.56818182 0.72463768 0.8       ]\n",
      "57 [0.59493671 0.72727273 0.23188406 0.08      ]\n",
      "58 [0.65822785 0.61363636 0.56521739 0.56      ]\n",
      "59 [0.84810127 0.75       0.82608696 0.84      ]\n",
      "60 [0.72151899 0.59090909 0.50724638 0.4       ]\n",
      "61 [0.83544304 0.68181818 0.63768116 0.56      ]\n",
      "62 [0.81012658 0.61363636 0.76811594 0.76      ]\n",
      "63 [0.7721519  0.65909091 0.68115942 0.56      ]\n",
      "64 [0.91139241 0.81818182 0.88405797 1.        ]\n",
      "65 [0.73417722 0.61363636 0.5942029  0.4       ]\n",
      "66 [0.91139241 0.72727273 0.86956522 0.72      ]\n",
      "67 [0.59493671 0.72727273 0.1884058  0.08      ]\n",
      "68 [0.73417722 0.61363636 0.56521739 0.48      ]\n",
      "69 [0.87341772 0.72727273 0.82608696 0.92      ]\n",
      "70 [0.75949367 0.77272727 0.65217391 0.64      ]\n",
      "71 [0.7721519  0.68181818 0.66666667 0.56      ]\n",
      "72 [0.62025316 0.70454545 0.2173913  0.08      ]\n",
      "73 [0.58227848 0.77272727 0.20289855 0.12      ]\n",
      "74 [0.72151899 0.63636364 0.5942029  0.52      ]\n",
      "75 [0.84810127 0.70454545 0.8115942  0.96      ]\n",
      "76 [0.82278481 0.68181818 0.84057971 0.88      ]\n",
      "77 [0.82278481 0.68181818 0.75362319 0.8       ]\n",
      "78 [0.84810127 0.75       0.82608696 1.        ]\n",
      "79 [0.88607595 0.72727273 0.68115942 0.56      ]\n",
      "80 [0.69620253 0.56818182 0.57971014 0.52      ]\n",
      "81 [0.74683544 0.68181818 0.60869565 0.6       ]\n",
      "82 [0.69620253 0.54545455 0.55072464 0.44      ]\n",
      "83 [0.63291139 0.79545455 0.23188406 0.24      ]\n",
      "84 [0.81012658 0.70454545 0.79710145 0.72      ]\n",
      "85 [0.81012658 0.65909091 0.62318841 0.52      ]\n",
      "86 [0.55696203 0.68181818 0.1884058  0.08      ]\n",
      "87 [0.5443038  0.68181818 0.15942029 0.04      ]\n",
      "88 [0.73417722 0.90909091 0.17391304 0.08      ]\n",
      "89 [0.75949367 0.65909091 0.65217391 0.6       ]\n",
      "90 [0.87341772 0.70454545 0.7826087  0.84      ]\n",
      "91 [0.72151899 1.         0.2173913  0.16      ]\n",
      "92 [0.91139241 0.68181818 0.84057971 0.64      ]\n",
      "93 [0.72151899 0.65909091 0.60869565 0.52      ]\n",
      "94 [0.7721519  0.68181818 0.71014493 0.72      ]\n",
      "95 [0.64556962 0.86363636 0.27536232 0.16      ]\n",
      "96 [0.81012658 0.63636364 0.8115942  0.84      ]\n",
      "97 [0.60759494 0.70454545 0.23188406 0.08      ]\n",
      "98 [0.63291139 0.75       0.20289855 0.08      ]\n",
      "99 [0.72151899 0.63636364 0.65217391 0.52      ]\n",
      "100 [0.73417722 0.61363636 0.73913043 0.76      ]\n",
      "101 [0.78481013 0.77272727 0.7826087  0.92      ]\n",
      "102 [0.79746835 0.65909091 0.8115942  0.72      ]\n",
      "103 [0.63291139 0.81818182 0.20289855 0.08      ]\n",
      "104 [0.79746835 0.52272727 0.63768116 0.52      ]\n",
      "105 [0.62025316 0.68181818 0.20289855 0.08      ]\n",
      "106 [0.7721519  0.63636364 0.68115942 0.48      ]\n",
      "107 [0.63291139 0.72727273 0.17391304 0.08      ]\n",
      "108 [0.55696203 0.65909091 0.20289855 0.08      ]\n",
      "109 [0.69620253 0.59090909 0.63768116 0.48      ]\n",
      "110 [0.62025316 0.70454545 0.2173913  0.04      ]\n",
      "111 [0.86075949 0.68181818 0.79710145 0.84      ]\n",
      "112 [0.97468354 0.59090909 1.         0.92      ]\n",
      "113 [0.6835443  0.77272727 0.24637681 0.08      ]\n",
      "114 [0.60759494 0.68181818 0.20289855 0.12      ]\n",
      "115 [0.63291139 0.52272727 0.47826087 0.4       ]\n",
      "116 [0.79746835 0.63636364 0.73913043 0.6       ]\n",
      "117 [0.84810127 0.70454545 0.68115942 0.6       ]\n",
      "118 [0.79746835 0.75       0.68115942 0.64      ]\n",
      "119 [0.69620253 0.54545455 0.53623188 0.4       ]\n",
      "120 [0.75949367 0.5        0.72463768 0.6       ]\n",
      "121 [0.64556962 0.79545455 0.20289855 0.12      ]\n",
      "122 [0.96202532 0.68181818 0.95652174 0.84      ]\n",
      "123 [0.84810127 0.68181818 0.75362319 0.92      ]\n",
      "124 [0.62025316 0.81818182 0.20289855 0.04      ]\n",
      "125 [0.64556962 0.56818182 0.43478261 0.44      ]\n",
      "126 [0.63291139 0.45454545 0.50724638 0.4       ]\n",
      "127 [0.79746835 0.56818182 0.72463768 0.76      ]\n",
      "128 [0.67088608 0.84090909 0.2173913  0.08      ]\n",
      "129 [0.87341772 0.70454545 0.73913043 0.92      ]\n",
      "130 [0.65822785 0.93181818 0.2173913  0.04      ]\n",
      "131 [0.78481013 0.5        0.65217391 0.6       ]\n",
      "132 [0.86075949 0.63636364 0.69565217 0.56      ]\n",
      "133 [0.79746835 0.61363636 0.71014493 0.72      ]\n",
      "134 [0.69620253 0.79545455 0.1884058  0.08      ]\n",
      "135 [0.64556962 0.84090909 0.2173913  0.16      ]\n",
      "136 [0.84810127 0.56818182 0.84057971 0.72      ]\n",
      "137 [0.58227848 0.81818182 0.14492754 0.08      ]\n",
      "138 [0.64556962 0.79545455 0.20289855 0.08      ]\n",
      "139 [0.72151899 0.86363636 0.24637681 0.12      ]\n",
      "140 [0.64556962 0.75       0.24637681 0.2       ]\n",
      "141 [0.75949367 0.68181818 0.69565217 0.72      ]\n",
      "142 [0.62025316 0.54545455 0.47826087 0.4       ]\n",
      "143 [0.70886076 0.56818182 0.56521739 0.44      ]\n",
      "144 [0.6835443  0.88636364 0.1884058  0.16      ]\n",
      "145 [0.92405063 0.65909091 0.91304348 0.72      ]\n",
      "146 [0.6835443  0.68181818 0.65217391 0.6       ]\n",
      "147 [0.83544304 0.65909091 0.66666667 0.52      ]\n",
      "148 [0.7721519  0.63636364 0.57971014 0.52      ]\n",
      "149 [0.70886076 0.61363636 0.60869565 0.52      ]\n"
     ]
    }
   ],
   "source": [
    "for i,sample in enumerate(X):\n",
    "    print(i,sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_dig = pd.read_csv('digit-recognizer/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_dig.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/PythonInterprener/venv/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y = ohe.fit_transform(np.array(y).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dig = train_dig/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_dig,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = littleNN(50)\n",
    "nn.fit(x_train.values,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9188095238095239"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = nn.predict(x_val.values)\n",
    "\n",
    "c = 0\n",
    "for i in range(res.shape[0]):\n",
    "    if(all(res[i]==y_val[i])):\n",
    "        c+=1\n",
    "\n",
    "c/res.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.83006340e-02, -7.11482790e-01, -6.43894464e-01, ...,\n",
       "         7.41371401e-01, -9.96892926e-01,  5.91131050e-02],\n",
       "       [-4.74998079e-02, -5.79528564e-01,  6.85188434e-01, ...,\n",
       "         1.77600649e-02,  9.09006657e-01,  7.38658106e-01],\n",
       "       [-2.65790629e-01,  1.77104299e-01, -6.75617219e-01, ...,\n",
       "         2.72290779e-01, -5.68197571e-01, -8.72056400e-01],\n",
       "       ...,\n",
       "       [ 1.11264375e-01, -4.15272516e-01, -6.82988075e-01, ...,\n",
       "         4.17429874e-01, -6.13697200e-01, -8.71148716e-01],\n",
       "       [ 4.01230085e-01, -2.13513531e-01,  9.57990630e-01, ...,\n",
       "        -5.43729585e-01,  6.87360689e-03, -5.09384329e-01],\n",
       "       [ 6.56590259e-01, -3.07171859e+00,  4.07859302e+00, ...,\n",
       "         9.31639699e+00,  1.22417041e+01, -8.28426765e+00]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
